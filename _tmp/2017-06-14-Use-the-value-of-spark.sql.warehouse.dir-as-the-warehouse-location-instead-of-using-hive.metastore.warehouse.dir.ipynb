{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坑的介绍在<a href=\"https://issues.apache.org/jira/browse/SPARK-15034\">这里</a>\n",
    "```shell\n",
    "从spark2.0开始，spark不再加载‘hive-site.xml'中的设置，也就是说，hive.metastore.warehouse.dir的设置无效。\n",
    "spark.sql.warehouse.dir的默认值为System.getProperty(\"user.dir\")/spark-warehouse，需要在spark的配置文件core-site.xml中设置\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://stackoverflow.com/questions/26545524/there-are-0-datanodes-running-and-no-nodes-are-excluded-in-this-operation\">这里</a>还有一个坑,\n",
    "遇到这种问题时需要清空$HADOOP_HOME/tmp里面的东西\n",
    "\n",
    "```shell\n",
    "$ rm -rf $HADOOP_HOME/tmp\n",
    "$ mkdir -p $HADOOP_HOME/tmp\n",
    "$ sudo chmod 750 $HADOOP_HOME/tmp\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
