<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/justttry.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/justttry.github.io/" rel="alternate" type="text/html" /><updated>2017-06-16T10:54:48+08:00</updated><id>http://localhost:4000/justttry.github.io/</id><title type="html">Justry</title><subtitle>A new Pythoner, a student of GOOGLE</subtitle><entry><title type="html">Don%27t Copy All Elements Of A Large Rdd To The Driver.</title><link href="http://localhost:4000/justttry.github.io/Don-27t-copy-all-elements-of-a-large-RDD-to-the-driver" rel="alternate" type="text/html" title="Don%27t Copy All Elements Of A Large Rdd To The Driver." /><published>2017-06-16T00:00:00+08:00</published><updated>2017-06-16T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/Don't-copy-all-elements-of-a-large-RDD-to-the-driver.</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/Don-27t-copy-all-elements-of-a-large-RDD-to-the-driver">&lt;p&gt;If your RDD is so large that all of it’s elements won’t fit in memory on the drive machine, don’t do this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myVeryLargeRDD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Collect will attempt to copy every single element in the RDD onto the single driver program, and then run out of memory and crash.&lt;/p&gt;

&lt;p&gt;Instead, you can make sure the number of elements you return is capped by calling take or takeSample, or perhaps filtering or sampling your RDD.&lt;/p&gt;

&lt;p&gt;Similarly, be cautious of these other actions as well unless you are sure your dataset size is small enough to fit in memory:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    1.countByKey
    2.countByValue
    3.collectAsMap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you really do need every one of these values of the RDD and the data is too big to fit into memory, you can write out the RDD to files or export the RDD to a database that is large enough to hold all the data.&lt;/p&gt;</content><author><name></name></author><summary type="html">If your RDD is so large that all of it’s elements won’t fit in memory on the drive machine, don’t do this:</summary></entry><entry><title type="html">内存溢出</title><link href="http://localhost:4000/justttry.github.io/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/" rel="alternate" type="text/html" title="内存溢出" /><published>2017-06-15T00:00:00+08:00</published><updated>2017-06-15T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/">&lt;p&gt;转来的&lt;a href=&quot;http://outofmemory.cn/c/java-outOfMemoryError&quot;&gt;文章&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;java.lang.OutOfMemoryError这个错误我相信大部分开发人员都有遇到过，产生该错误的原因大都出于以下原因：JVM内存过小、程序不严密，产生了过多的垃圾。

导致OutOfMemoryError异常的常见原因有以下几种：

1.内存中加载的数据量过于庞大，如一次从数据库取出过多数据；
2.集合类中有对对象的引用，使用完后未清空，使得JVM不能回收；
3.代码中存在死循环或循环产生过多重复的对象实体；
4.使用的第三方软件中的BUG；
5.启动参数内存值设定的过小；

此错误常见的错误提示：

1.tomcat:java.lang.OutOfMemoryError: PermGen space
2.tomcat:java.lang.OutOfMemoryError: Java heap space
3.weblogic:Root cause of ServletException java.lang.OutOfMemoryError
4.resin:java.lang.OutOfMemoryError
5.java:java.lang.OutOfMemoryError
解决java.lang.OutOfMemoryError的方法有如下几种：
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">转来的文章</summary></entry><entry><title type="html">Hive不能多进程操作的问题</title><link href="http://localhost:4000/justttry.github.io/HIVE%E4%B8%8D%E8%83%BD%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%93%8D%E4%BD%9C%E7%9A%84%E9%97%AE%E9%A2%98/" rel="alternate" type="text/html" title="Hive不能多进程操作的问题" /><published>2017-06-15T00:00:00+08:00</published><updated>2017-06-15T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/HIVE%E4%B8%8D%E8%83%BD%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%93%8D%E4%BD%9C%E7%9A%84%E9%97%AE%E9%A2%98</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/HIVE%E4%B8%8D%E8%83%BD%E5%A4%9A%E8%BF%9B%E7%A8%8B%E6%93%8D%E4%BD%9C%E7%9A%84%E9%97%AE%E9%A2%98/">&lt;p&gt;笔者目前遇到一个问题，当开启两个ipython进程的时候会遇到下列错误&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Failed to start database &lt;span class=&quot;s1&quot;&gt;'metastore_db'&lt;/span&gt; with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader&lt;span class=&quot;nv&quot;&gt;$$&lt;/span&gt;anon&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;@13015ec0, see the next exception &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;details.
org.datanucleus.exceptions.NucleusDataStoreException: Failed to start database &lt;span class=&quot;s1&quot;&gt;'metastore_db'&lt;/span&gt; with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader&lt;span class=&quot;nv&quot;&gt;$$&lt;/span&gt;anon&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;@13015ec0, see the next exception &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;details.
at org.datanucleus.store.rdbms.ConnectionFactoryImpl&lt;span class=&quot;nv&quot;&gt;$ManagedConnectionImpl&lt;/span&gt;.getConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ConnectionFactoryImpl.java:516&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.store.rdbms.RDBMSStoreManager.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RDBMSStoreManager.java:298&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NativeConstructorAccessorImpl.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DelegatingConstructorAccessorImpl.java:45&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.reflect.Constructor.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Constructor.java:526&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NonManagedPluginRegistry.java:631&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.plugin.PluginManager.createExecutableExtension&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;PluginManager.java:301&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.NucleusContext.createStoreManagerForProperties&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NucleusContext.java:1187&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.NucleusContext.initialise&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NucleusContext.java:356&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOPersistenceManagerFactory.java:775&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOPersistenceManagerFactory.java:333&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOPersistenceManagerFactory.java:202&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke0&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NativeMethodAccessorImpl.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.DelegatingMethodAccessorImpl.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DelegatingMethodAccessorImpl.java:43&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.reflect.Method.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Method.java:606&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at javax.jdo.JDOHelper&lt;span class=&quot;nv&quot;&gt;$16&lt;/span&gt;.run&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOHelper.java:1965&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.security.AccessController.doPrivileged&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at javax.jdo.JDOHelper.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOHelper.java:1960&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOHelper.java:1166&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at javax.jdo.JDOHelper.getPersistenceManagerFactory&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOHelper.java:808&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at javax.jdo.JDOHelper.getPersistenceManagerFactory&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;JDOHelper.java:701&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.ObjectStore.getPMF&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ObjectStore.java:365&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ObjectStore.java:394&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.ObjectStore.initialize&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ObjectStore.java:291&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.ObjectStore.setConf&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ObjectStore.java:258&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.util.ReflectionUtils.setConf&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ReflectionUtils.java:73&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.util.ReflectionUtils.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ReflectionUtils.java:133&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RawStoreProxy.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RawStoreProxy.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RawStoreProxy.java:66&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStore&lt;span class=&quot;nv&quot;&gt;$HMSHandler&lt;/span&gt;.newRawStore&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStore.java:593&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStore&lt;span class=&quot;nv&quot;&gt;$HMSHandler&lt;/span&gt;.getMS&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStore.java:571&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStore&lt;span class=&quot;nv&quot;&gt;$HMSHandler&lt;/span&gt;.createDefaultDB&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStore.java:620&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStore&lt;span class=&quot;nv&quot;&gt;$HMSHandler&lt;/span&gt;.init&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStore.java:461&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RetryingHMSHandler.java:66&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RetryingHMSHandler.java:72&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStore.java:5762&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveMetaStoreClient.java:199&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;SessionHiveMetaStoreClient.java:74&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NativeConstructorAccessorImpl.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DelegatingConstructorAccessorImpl.java:45&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.reflect.Constructor.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Constructor.java:526&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;MetaStoreUtils.java:1521&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RetryingMetaStoreClient.java:86&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RetryingMetaStoreClient.java:132&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;RetryingMetaStoreClient.java:104&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hive.java:3005&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.Hive.getMSC&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hive.java:3024&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hive.java:1234&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hive.java:174&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.metadata.Hive.&amp;lt;clinit&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hive.java:166&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.hadoop.hive.ql.session.SessionState.start&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;SessionState.java:503&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ClientWrapper.scala:171&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NativeConstructorAccessorImpl.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DelegatingConstructorAccessorImpl.java:45&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.reflect.Constructor.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Constructor.java:526&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;IsolatedClientLoader.scala:183&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.client.IsolatedClientLoader.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;IsolatedClientLoader.scala:179&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.HiveContext.metadataHive&lt;span class=&quot;nv&quot;&gt;$lzycompute&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveContext.scala:227&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.HiveContext.metadataHive&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveContext.scala:186&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.HiveContext.setConf&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveContext.scala:393&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.HiveContext.defaultOverrides&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveContext.scala:175&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HiveContext.scala:178&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance0&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.NativeConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;NativeConstructorAccessorImpl.java:57&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DelegatingConstructorAccessorImpl.java:45&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.reflect.Constructor.newInstance&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Constructor.java:526&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.reflection.MethodInvoker.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;MethodInvoker.java:234&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.reflection.ReflectionEngine.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ReflectionEngine.java:379&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.Gateway.invoke&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Gateway.java:214&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.commands.ConstructorCommand.invokeConstructor&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ConstructorCommand.java:79&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.commands.ConstructorCommand.execute&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ConstructorCommand.java:68&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at py4j.GatewayConnection.run&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;GatewayConnection.java:207&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.lang.Thread.run&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Thread.java:745&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Caused by: java.sql.SQLException: Failed to start database &lt;span class=&quot;s1&quot;&gt;'metastore_db'&lt;/span&gt; with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader&lt;span class=&quot;nv&quot;&gt;$$&lt;/span&gt;anon&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;@13015ec0, see the next exception &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;details.
at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.Util.newEmbedSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.Util.seeNextException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.EmbedConnection.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.EmbedConnection40.&amp;lt;init&amp;gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.jdbc.Driver40.getNewEmbedConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.jdbc.InternalDriver.connect&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.jdbc.Driver20.connect&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.jdbc.AutoloadedDriver.connect&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.sql.DriverManager.getConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DriverManager.java:571&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.sql.DriverManager.getConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DriverManager.java:187&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.commons.dbcp.DriverManagerConnectionFactory.createConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;DriverManagerConnectionFactory.java:78&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.commons.dbcp.PoolableConnectionFactory.makeObject&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;PoolableConnectionFactory.java:582&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.commons.pool.impl.GenericObjectPool.borrowObject&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;GenericObjectPool.java:1148&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.commons.dbcp.PoolingDataSource.getConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;PoolingDataSource.java:106&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.datanucleus.store.rdbms.ConnectionFactoryImpl&lt;span class=&quot;nv&quot;&gt;$ManagedConnectionImpl&lt;/span&gt;.getConnection&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ConnectionFactoryImpl.java:501&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
... 76 more
Caused by: java.sql.SQLException: Failed to start database &lt;span class=&quot;s1&quot;&gt;'metastore_db'&lt;/span&gt; with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader&lt;span class=&quot;nv&quot;&gt;$$&lt;/span&gt;anon&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;@13015ec0, see the next exception &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;details.
at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
... 93 more
Caused by: java.sql.SQLException: Another instance of Derby may have already booted the database /home/dmytro/work/shared_store/integration/metastore_db.
at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.jdbc.Util.generateCsSQLException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
... 90 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /home/dmytro/work/shared_store/integration/metastore_db.
at org.apache.derby.iapi.error.StandardException.newException&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at java.security.AccessController.doPrivileged&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Native Method&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.TopService.bootModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.startModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.raw.RawStore.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.TopService.bootModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.startModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.store.access.RAMAccessManager.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.TopService.bootModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.startModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.db.BasicDatabase.bootStore&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.db.BasicDatabase.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.boot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.TopService.bootModule&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.bootService&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Unknown Source&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
... 90 more
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;很显然，提示’metastore_db’冲突。
解决方法：&lt;/p&gt;

&lt;p&gt;1.消除’metastore_db’锁死问题&lt;/p&gt;

&lt;p&gt;2.不同进程指向不同的’metastore_db’路径&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-10872&quot;&gt;这里&lt;/a&gt;是方法一:&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;删除文件夹metastore_db中的dbex.lck文件即可
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于方法二，目前还没有找到解决途径，2.0.0以后的spark版本已经弃用了hive-site.xml设置。&lt;/p&gt;</content><author><name></name></author><summary type="html">笔者目前遇到一个问题，当开启两个ipython进程的时候会遇到下列错误</summary></entry><entry><title type="html">Use The Value Of Spark.sql.warehouse.dir As The Warehouse Location Instead Of Using Hive.metastore.warehouse.dir</title><link href="http://localhost:4000/justttry.github.io/Use-the-value-of-spark.sql.warehouse.dir-as-the-warehouse-location-instead-of-using-hive.metastore.warehouse.dir/" rel="alternate" type="text/html" title="Use The Value Of Spark.sql.warehouse.dir As The Warehouse Location Instead Of Using Hive.metastore.warehouse.dir" /><published>2017-06-14T00:00:00+08:00</published><updated>2017-06-14T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/Use-the-value-of-spark.sql.warehouse.dir-as-the-warehouse-location-instead-of-using-hive.metastore.warehouse.dir</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/Use-the-value-of-spark.sql.warehouse.dir-as-the-warehouse-location-instead-of-using-hive.metastore.warehouse.dir/">&lt;p&gt;坑的介绍在&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-15034&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;从spark2.0开始，spark不再加载‘hive-site.xml&lt;span class=&quot;s1&quot;&gt;'中的设置，也就是说，hive.metastore.warehouse.dir的设置无效。
spark.sql.warehouse.dir的默认值为System.getProperty(&quot;user.dir&quot;)/spark-warehouse，需要在spark的配置文件core-site.xml中设置
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/26545524/there-are-0-datanodes-running-and-no-nodes-are-excluded-in-this-operation&quot;&gt;这里&lt;/a&gt;还有一个坑,
遇到这种问题时需要清空$HADOOP_HOME/tmp里面的东西&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rm -rf &lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/tmp
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p &lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/tmp
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo chmod 750 &lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pyspark --master yarn --deploy-mode client --num-executors 7 --executor-cores 2 --conf spark.sql.warehouse.dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hdfs://user/hive/warehouse
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">坑的介绍在这里 从spark2.0开始，spark不再加载‘hive-site.xml'中的设置，也就是说，hive.metastore.warehouse.dir的设置无效。 spark.sql.warehouse.dir的默认值为System.getProperty(&quot;user.dir&quot;)/spark-warehouse，需要在spark的配置文件core-site.xml中设置</summary></entry><entry><title type="html">Hive安装</title><link href="http://localhost:4000/justttry.github.io/HIVE%E5%AE%89%E8%A3%85/" rel="alternate" type="text/html" title="Hive安装" /><published>2017-06-13T00:00:00+08:00</published><updated>2017-06-13T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/HIVE%E5%AE%89%E8%A3%85</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/HIVE%E5%AE%89%E8%A3%85/">&lt;h1 id=&quot;1安装java&quot;&gt;1.安装JAVA&lt;/h1&gt;

&lt;h2 id=&quot;a安装过程省略&quot;&gt;(a)安装过程省略&lt;/h2&gt;
&lt;p&gt;过程参考&lt;a href=&quot;https://www.tutorialspoint.com/hive/hive_installation.htm&quot;&gt;这里&lt;/a&gt;, 版本略旧。&lt;/p&gt;

&lt;h2 id=&quot;b查看安装版本&quot;&gt;(b)查看安装版本&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;java –version

java version &lt;span class=&quot;s2&quot;&gt;&quot;1.8.0_131&quot;&lt;/span&gt;
Java&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;TM&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; SE Runtime Environment &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;build 1.8.0_131-b11&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Java HotSpot&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;TM&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 64-Bit Server VM &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;build 25.131-b11, mixed mode
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;2安装hadoop&quot;&gt;2.安装HADOOP&lt;/h1&gt;

&lt;h2 id=&quot;a安装过程省略-1&quot;&gt;(a)安装过程省略&lt;/h2&gt;

&lt;h2 id=&quot;b查看安装版本-1&quot;&gt;(b)查看安装版本&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;hadoop version

Hadoop 2.8.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r 91f2b7a13d1e97be65db92ddabc627cc29ac0009
Compiled by jdu on 2017-03-17T04:12Z
Compiled with protoc 2.5.0
From &lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;with checksum 60125541c2b3e266cbf3becc5bda666
This &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;was run using /home/ubuntu/Download/hadoop-2.8.0/share/hadoop/common/hadoop-common-2.8.0.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;c-启动yarn集群&quot;&gt;(c) 启动YARN集群&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-dfs.sh
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-yarn.sh
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;3安装spark&quot;&gt;3.安装SPARK&lt;/h1&gt;

&lt;h2 id=&quot;a下载spark源码省略&quot;&gt;(a).下载SPARK源码，省略&lt;/h2&gt;

&lt;h2 id=&quot;b编译&quot;&gt;(b).编译&lt;/h2&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;spark-2.1.1/
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;./build/mvn -Phadoop-provided -Pyarn -Phadoop-2.7 -Phive -Phive-thriftserver -DskipTests clean package

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Reactor Summary:
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] 
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Parent POM ........................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 13.663 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Tags ................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 17.955 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Sketch ............................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  6.808 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Networking ........................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 11.276 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Shuffle Streaming Service ............ SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  5.935 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Unsafe ............................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  9.713 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Launcher ............................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 14.214 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Core ................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;02:28 min]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project ML Local Library ..................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 11.149 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project GraphX ............................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 13.799 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Streaming ............................ SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 32.115 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Catalyst ............................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;01:20 min]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project SQL .................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;01:45 min]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project ML Library ........................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;01:15 min]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Tools ................................ SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  1.924 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Hive ................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;01:00 min]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project REPL ................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  5.636 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project YARN Shuffle Service ................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  8.612 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project YARN ................................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 15.541 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Hive Thrift Server ................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 29.356 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Assembly ............................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 10.835 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project External Flume Sink .................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  7.775 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project External Flume ....................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  9.472 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project External Flume Assembly .............. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  2.272 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Integration &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Kafka 0.8 .................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 23.306 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Examples ............................. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 18.441 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project External Kafka Assembly .............. SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  4.008 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Integration &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Kafka 0.10 ................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; 11.553 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Integration &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Kafka 0.10 Assembly .......... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  3.748 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Kafka 0.10 Source &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Structured Streaming ......... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  9.846 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Spark Project Java 8 Tests ......................... SUCCESS &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;  5.179 s]
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] ------------------------------------------------------------------------
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] BUILD SUCCESS
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] ------------------------------------------------------------------------
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Total &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;: 12:59 min
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Finished at: 2017-06-13T06:11:22+00:00
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] Final Memory: 96M/1352M
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;INFO] ------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;4安装hive&quot;&gt;4.安装HIVE&lt;/h1&gt;

&lt;h2 id=&quot;a安装hive&quot;&gt;(a).安装HIVE&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;wget https://archive.apache.org/dist/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;tar zxvf apache-hive-1.2.1-bin.tar.gz
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ls
apache-hive-1.2.1-bin  apache-hive-1.2.1-bin.tar.gz
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo mv apache-hive-1.2.1-bin /usr/local/hive
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;注意版本，根据SPARK的版本选择HIVE版本，否则会出错。&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;b配置hive环境&quot;&gt;(b).配置HIVE环境&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi .bashrc

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HIVE_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/hive
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$HIVE_HOME&lt;/span&gt;/bin
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CLASSPATH&lt;/span&gt;:/usr/local/Hadoop/lib/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;:.
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CLASSPATH&lt;/span&gt;:/usr/local/hive/lib/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;:.

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; .bashrc
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;c配置hive&quot;&gt;(c).配置HIVE&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HIVE_HOME&lt;/span&gt;/conf
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cp hive-env.sh.template hive-env.sh
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi hive-env.sh

&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;HADOOP_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/hadoop

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cp hive-default.xml.template hive-site.xml
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi hive-site.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;配置服务器端口,介绍在&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/HiveDerbyServerMode&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;jdbc:derby://localhost:1527/metastore_db;create&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;JDBC connect string &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;a JDBC metastore &amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;org.apache.derby.jdbc.ClientDriver&amp;lt;/value&amp;gt;
    &amp;lt;description&amp;gt;Driver class name &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;a JDBC metastore&amp;lt;/description&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;在hive-site.xml配置的开始行添加下列代码，原因在&lt;a href=&quot;https://stackoverflow.com/questions/28536340/hive-shell-not-opening-when-i-have-hive-site-xml&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;system:java.io.tmpdir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;/tmp/hive/java&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;system:user.name&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.name&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi jpox.properties

javax.jdo.PersistenceManagerFactoryClass&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;org.jpox.PersistenceManagerFactoryImpl
org.jpox.autoCreateSchema&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;org.jpox.validateTables&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;org.jpox.validateColumns&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;org.jpox.validateConstraints&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;org.jpox.storeManagerType&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rdbms
org.jpox.autoCreateSchema&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;org.jpox.autoStartMechanismMode&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;checked
org.jpox.transactionIsolation&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;read_committed
javax.jdo.option.DetachAllOnCommit&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;javax.jdo.option.NontransactionalRead&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;javax.jdo.option.ConnectionDriverName&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;org.apache.derby.jdbc.ClientDriver
javax.jdo.option.ConnectionURL&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;jdbc:derby://localhost:1527/metastore_db;create&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;javax.jdo.option.ConnectionUserName&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;APP
javax.jdo.option.ConnectionPassword&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mine

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;5安装derby&quot;&gt;5.安装DERBY&lt;/h1&gt;

&lt;h2 id=&quot;a安装derby&quot;&gt;(a).安装DERBY&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;wget http://archive.apache.org/dist/db/derby/db-derby-10.10.2.0/db-derby-10.10.2.0-bin.tar.gz
版本和HIVE保持一致
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;tar zxvf db-derby-10.10.2.0-bin.tar.gz
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ls
db-derby-10.10.2.0-bin db-derby-10.10.2.0-bin.tar.gz
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo mv db-derby-10.10.2.0-bin /usr/local/derby
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;b配置derby环境&quot;&gt;(b).配置DERBY环境&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi .bashrc
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;DERBY_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/derby
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/bin
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CLASSPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$CLASSPATH&lt;/span&gt;:&lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/lib/derby.jar:&lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/lib/derbytools.jar
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; .bashrc
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cp &lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/lib/derbyclient.jar &lt;span class=&quot;nv&quot;&gt;$HIVE_HOME&lt;/span&gt;/lib/
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cp &lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/lib/derbytools.jar &lt;span class=&quot;nv&quot;&gt;$HIVE_HOME&lt;/span&gt;/lib/
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir &lt;span class=&quot;nv&quot;&gt;$DERBY_HOME&lt;/span&gt;/data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;6验证hive&quot;&gt;6.验证HIVE&lt;/h1&gt;

&lt;h2 id=&quot;a验证derby&quot;&gt;(a).验证DERBY&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;java org.apache.derby.tools.sysinfo

------------------ Java Information ------------------
Java Version:    1.8.0_131
Java Vendor:     Oracle Corporation
Java home:       /usr/lib/jvm/java-8-oracle/jre
Java classpath:  :/usr/local/Hadoop/lib/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;:.:/usr/local/hive/lib/calcite-core-1.2.0-incubating.jar::/usr/local/derby/lib/derbytools.jar
OS name:         Linux
OS architecture: amd64
OS version:      4.4.0-1018-aws
Java user name:  ubuntu
Java user home:  /home/ubuntu
Java user dir:   /home/ubuntu
java.specification.name: Java Platform API Specification
java.specification.version: 1.8
java.runtime.version: 1.8.0_131-b11
--------- Derby Information --------
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/hive/lib/derby-10.10.2.0.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/hive/lib/derbytools.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/hive/lib/derbyclient.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/hive/lib/hive-jdbc-1.2.1-standalone.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/derby/lib/derby.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;/usr/local/derby/lib/derbytools.jar] 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
------------------------------------------------------
----------------- Locale Information -----------------
Current Locale :  &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;English/United States &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;en_US]]
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cs]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;de_DE]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;es]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;fr]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;hu]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;it]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ja_JP]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ko_KR]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;pl]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;pt_BR]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ru]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;zh_CN]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Found support &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;locale: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;zh_TW]
     version: 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
------------------------------------------------------
------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;b启动yarn集群&quot;&gt;(b).启动YARN集群&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-dfs.sh
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-yarn.sh
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HADOOP_HOME&lt;/span&gt;/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;c启动derby服务器&quot;&gt;(c).启动DERBY服务器&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;startNetworkServer
Tue Jun 13 06:51:35 UTC 2017 : Security manager installed using the Basic server security policy.
Tue Jun 13 06:51:35 UTC 2017 : Apache Derby Network Server - 10.10.2.0 - &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1582446&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; started and ready to accept connections on port 1527

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;b启动hive&quot;&gt;(b).启动HIVE&lt;/h2&gt;
&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;hive
Logging initialized using configuration &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;jar:file:/usr/local/hive/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">1.安装JAVA</summary></entry><entry><title type="html">Yarn Maximum Allocation Mb</title><link href="http://localhost:4000/justttry.github.io/YARN-maximum-allocation-mb/" rel="alternate" type="text/html" title="Yarn Maximum Allocation Mb" /><published>2017-06-09T00:00:00+08:00</published><updated>2017-06-09T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/YARN-maximum-allocation-mb</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/YARN-maximum-allocation-mb/">&lt;h3 id=&quot;报错如下&quot;&gt;报错如下：&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;[Stage 2:=============================&amp;gt;                             (1 + 1) / 2]17/06/09 06:48:54 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1496990795119_0001_02_000002 on host: slave1. Exit status: -1000. Diagnostics: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073744745_3921 file=/user/ubuntu/.sparkStaging/application_1496990795119_0001/py4j-0.10.4-src.zip
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073744745_3921 file=/user/ubuntu/.sparkStaging/application_1496990795119_0001/py4j-0.10.4-src.zip &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The problem is more likely a lack of correlation between Spark’s request for RAM (driver memory + executor memory) and Yarn’s container sizing configuration. Yarn settings determine min/max container sizes, and should be based on available physical memory, number of nodes, etc. As a rule of thumb, try making the minimum Yarn container size 1.5 times the size of the requested driver/executor memory (in this case, 1.5 GB).&lt;/p&gt;

&lt;h3 id=&quot;修改设置如下&quot;&gt;修改设置如下：&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &amp;lt;property&amp;gt;  
        &amp;lt;name&amp;gt;yarn.nodemanager.resource.memory-mb&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;64512&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt; 
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.scheduler.maximum-allocation-mb&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;51200&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.resource.cpu-vcores&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;6&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">报错如下： [Stage 2:=============================&amp;gt; (1 + 1) / 2]17/06/09 06:48:54 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1496990795119_0001_02_000002 on host: slave1. Exit status: -1000. Diagnostics: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073744745_3921 file=/user/ubuntu/.sparkStaging/application_1496990795119_0001/py4j-0.10.4-src.zip org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073744745_3921 file=/user/ubuntu/.sparkStaging/application_1496990795119_0001/py4j-0.10.4-src.zip</summary></entry><entry><title type="html">Replicas Less</title><link href="http://localhost:4000/justttry.github.io/Replicas-less/" rel="alternate" type="text/html" title="Replicas Less" /><published>2017-06-09T00:00:00+08:00</published><updated>2017-06-09T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/Replicas-less</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/Replicas-less/">&lt;h3 id=&quot;报错如下&quot;&gt;报错如下：&lt;/h3&gt;
&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;17/06/09 10:21:42 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 1.0 (TID 4, slave1, executor 4): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073746824_6000 file=/user/ubuntu/data_path/training_set.parquet/part-00005-4499448f-ac1c-41bd-940c-448f6174663d.snappy.parquet&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;输入命令&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    hadoop fsck / -files -blocks -locations
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;可以看到&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/user/ubuntu/atlas_higgs.csv 55253673 bytes, 1 block(s):  Under replicated BP-1759922210-172.31.7.59-1496717496059:blk_1073746805_5981. Target Replicas is 2 but found 1 live replica(s), 0 decommissioned replica(s) and 0 decommissioning replica(s).
0. BP-1759922210-172.31.7.59-1496717496059:blk_1073746805_5981 len=55253673 Live_repl=1 [DatanodeInfoWithStorage[172.31.9.28:50010,DS-65d96fa8-c2e4-4783-a32a-bf7ede3b9115,DISK]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The replication count for files submitted as part of your job is controlled by the parameter mapreduce.client.submit.file.replication or mapred.submit.replication in mapred-site.xml. You can adjust this down for clusters that are smaller than 10 nodes.&lt;/p&gt;

&lt;p&gt;修改设置如下：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapreduce.client.submit.file.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;2&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;依然不起作用&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt;重新格式化namenode之后问题消失。可能是由于当前hadoop节点和之前的配置不同导致的。&lt;/span&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">报错如下： 17/06/09 10:21:42 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 1.0 (TID 4, slave1, executor 4): org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1759922210-172.31.7.59-1496717496059:blk_1073746824_6000 file=/user/ubuntu/data_path/training_set.parquet/part-00005-4499448f-ac1c-41bd-940c-448f6174663d.snappy.parquet</summary></entry><entry><title type="html">使用wingide进行远程调试</title><link href="http://localhost:4000/justttry.github.io/%E4%BD%BF%E7%94%A8Wingide%E8%BF%9B%E8%A1%8C%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/" rel="alternate" type="text/html" title="使用wingide进行远程调试" /><published>2017-06-02T00:00:00+08:00</published><updated>2017-06-02T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/%E4%BD%BF%E7%94%A8Wingide%E8%BF%9B%E8%A1%8C%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/%E4%BD%BF%E7%94%A8Wingide%E8%BF%9B%E8%A1%8C%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/">&lt;h3 id=&quot;wingide远程调试架构如下&quot;&gt;wingide远程调试架构如下&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://justttry.github.io/images/Wingide_remote_debug.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;See also the Manually Configured Remote Debugging Example.&lt;/p&gt;

&lt;p&gt;(1) First set up Wing to successfully accept connections from another process within the same machine, as described in section Importing the Debugger. You can use any Python script for testing this until you have values that work.&lt;/p&gt;

&lt;p&gt;(2) Optionally, alter the Server Host preference to the name or IP address of the network interface on which the IDE listens for debug connections. The default server is None, which indicates that the IDE should listen on all the valid network interfaces on the host.&lt;/p&gt;

&lt;p&gt;(3) Optionally, alter the preference Server Port to the TCP/IP port on which the IDE should listen for debug connections. This value may need to be changed if multiple copies of Wing are running on the same host.&lt;/p&gt;

&lt;p&gt;(4) Set the Allowed Hosts preference to include the host on which the debug process will be run. For security purposes, Wing will reject connections if the host isn’t included here.&lt;/p&gt;

&lt;p&gt;(5) Configure any firewall on the system that Wing is running on to accept a connection on the server port from the system that the debug process will run on.&lt;/p&gt;

&lt;p&gt;(6) Next install Wing on the machine on which you plan to run your debug program. Creating an entire Wing installation is the easiest approach. Alternatives are to copy only the debug server code out of a Wing installation on the same type of OS or to compile the debugger core from source code. For details, see Manually Installing the Debugger Core.&lt;/p&gt;

&lt;p&gt;(7) Next, transfer copies of all your debug code so that the source files are available on the host where Wing will be running and at least the *.pyc files are available on the debug host.&lt;/p&gt;

&lt;p&gt;During debugging, the client and server copies of your source files must match or the debugger will either fail to stop at breakpoints or stop at the wrong place, and stepping through code may not work properly.&lt;/p&gt;

&lt;p&gt;You will need to use Samba, FTP, NFS, or some other file sharing or file transfer mechanism to keep the remote files up to date as you edit them in Wing.&lt;/p&gt;

&lt;p&gt;If files appear in different disk locations on the two machines, you will also need to set up a file location map, as described in Manually Configured File Location Maps.&lt;/p&gt;

&lt;p&gt;(8) On your debug host, copy wingdbstub.py into the same directory as your source files and import it in your Python source as described in Debugging Externally Launched Code.&lt;/p&gt;

&lt;p&gt;(9) If you didn’t copy wingdbstub.py out of a complete installation of Wing on the debug host, or if working on OS X or with the zip file or tar file installations of Wing, you will need to set WINGHOME in your copy to match the location where you have copied the debug server code on your debug host.&lt;/p&gt;

&lt;p&gt;(10) In wingdbstub.py on your debug host, set kWingHostPort. The host in this value must be the IP address of the machine where Wing is running. The port must match the port configured with the Server Port preference on the host where Wing is running.&lt;/p&gt;

&lt;p&gt;(11) Then restart Wing and try running your program on the debug host. You should see the Wing debugger status icon change to indicate that a debug process has attached.&lt;/p&gt;

&lt;p&gt;If you have problems making this work, try setting kLogFile variable in wingdbstub.py for log additional diagnostic information.&lt;/p&gt;

&lt;h3 id=&quot;下面有一个使用windows-wing来调试linux代码的例子&quot;&gt;下面有一个使用windows wing来调试linux代码的例子&lt;/h3&gt;

&lt;p&gt;On the Windows machine, the following preferences must be specified:&lt;/p&gt;

&lt;p&gt;(1)Accept Debug Connections should be checked &lt;br /&gt;
(2)Server Host should be set to All Interfaces (this is the default)&lt;br /&gt;
(3)Server Port should be set to 50005 (this is the default)&lt;br /&gt;
(4)Allowed Hosts should be altered by adding 192.168.1.200&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;On the Linux/Unix machine, the following value is needed in wingdbstub.py:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://justttry.github.io/images/wingide_configure.png&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">wingide远程调试架构如下</summary></entry><entry><title type="html">Not An Element Of Tensor Graph</title><link href="http://localhost:4000/justttry.github.io/not-an-element-of-Tensor-graph/" rel="alternate" type="text/html" title="Not An Element Of Tensor Graph" /><published>2017-05-24T00:00:00+08:00</published><updated>2017-05-24T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/not-an-element-of-Tensor-graph</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/not-an-element-of-Tensor-graph/">&lt;p&gt;错误:&lt;span style=&quot;color:red&quot;&gt;TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(“Placeholder_2:0”, shape=(500, 500), dtype=float32) is not an element of this graph.&lt;/span&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Exception in thread Thread-2:                                                   
Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/threading.py&quot;, line 801, in __bootstrap_inner
    self.run()
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/threading.py&quot;, line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/distkeras/job_deployment.py&quot;, line 281, in run
    self.read_trained_model()
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/distkeras/job_deployment.py&quot;, line 207, in read_trained_model
    self.trained_model = deserialize_keras_model(unpickle_object(f.read()))
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/distkeras/utils.py&quot;, line 127, in deserialize_keras_model
    model.set_weights(weights)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/models.py&quot;, line 700, in set_weights
    self.model.set_weights(weights)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py&quot;, line 1973, in set_weights
    K.batch_set_value(tuples)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py&quot;, line 2153, in batch_set_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py&quot;, line 778, in run
    run_metadata_ptr)
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py&quot;, line 933, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(&quot;Placeholder_2:0&quot;, shape=(500, 500), dtype=float32) is not an element of this graph.

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;错误原因：多线程、分布式环境下，恢复Model时的Tensor Graph和生成Model时不同。&lt;/p&gt;

&lt;p&gt;解决方法：将生成Model的Tesor Graph复制到恢复线程(或者DataNode)中。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Env:Ubuntu 16.4
    spark
    keras
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;p&gt;1.Right after loading or constructing your model, save the TensorFlow graph:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.In the other thread (or perhaps in an asynchronous event handler), do:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I learned about this from https://github.com/fchollet/keras/issues/2397&lt;/p&gt;

&lt;h2 id=&quot;具体操作如下&quot;&gt;具体操作如下:&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## main.py&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;deployment&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## deployment.py&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Job&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_finished&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;global&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;        
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inference&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">错误:TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(“Placeholder_2:0”, shape=(500, 500), dtype=float32) is not an element of this graph.</summary></entry><entry><title type="html">Hadoop Import Error</title><link href="http://localhost:4000/justttry.github.io/Hadoop-Import-Error/" rel="alternate" type="text/html" title="Hadoop Import Error" /><published>2017-05-19T00:00:00+08:00</published><updated>2017-05-19T00:00:00+08:00</updated><id>http://localhost:4000/justttry.github.io/Hadoop-Import-Error</id><content type="html" xml:base="http://localhost:4000/justttry.github.io/Hadoop-Import-Error/">&lt;p&gt;错误如下&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;nd = serializer.loads(command.value)&lt;br /&gt;
  File “/home/ubuntu/Download/spark-2.1.1/python/pyspark/serializers.py”, line 454, in loads
    return pickle.loads(obj)&lt;br /&gt;
  File “/home/ubuntu/anaconda2/lib/python2.7/site-packages/distkeras/workers.py”, line 13, in &lt;module&gt;
    from distkeras.utils import deserialize_keras_model&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/distkeras/utils.py&quot;, line 5, in &lt;module&gt;
    from keras import backend as K&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/__init__.py&quot;, line 3, in &lt;module&gt;
    from . import activations&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/activations.py&quot;, line 4, in &lt;module&gt;
    from . import backend as K&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/__init__.py&quot;, line 73, in &lt;module&gt;
    from .tensorflow_backend import *&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py&quot;, line 1, in &lt;module&gt;
    import tensorflow as tf&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/__init__.py&quot;, line 24, in &lt;module&gt;
    from tensorflow.python import *&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/python/__init__.py&quot;, line 54, in &lt;module&gt;
    from tensorflow.core.framework.graph_pb2 import *&lt;br /&gt;
  File &quot;/home/ubuntu/anaconda2/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py&quot;, line 6, in &lt;module&gt;&lt;br /&gt;
    from google.protobuf import descriptor as _descriptor&lt;br /&gt;
&lt;span style=&quot;color:red&quot;&gt;ImportError&lt;/span&gt;: No module named google.protobuf&lt;br /&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/module&gt;&lt;/p&gt;

&lt;p&gt;1.确定需要导入的库是否正确安装&lt;br /&gt;
2.PYTHONPATH中是否指定路径&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PYTHONPATH=/home/ubuntu/Download/spark-2.1.1/python:/home/ubuntu/anaconda2/lib/python2.7/site-packages
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;There is another possibility, if you are running a python 2.7.11 or other similar versions,&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    sudo pip install protobuf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;is ok. But if you are in a anaconda environment, you should use&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    sudo conda install protobuf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">错误如下</summary></entry></feed>