
<p>If your RDD is so large that all of it’s elements won’t fit in memory on the drive machine, don’t do this:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">values</span> <span class="o">=</span> <span class="n">myVeryLargeRDD</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</code></pre>
</div>

<p>Collect will attempt to copy every single element in the RDD onto the single driver program, and then run out of memory and crash.</p>

<p>Instead, you can make sure the number of elements you return is capped by calling take or takeSample, or perhaps filtering or sampling your RDD.</p>

<p>Similarly, be cautious of these other actions as well unless you are sure your dataset size is small enough to fit in memory:</p>

<div class="language-shell highlighter-rouge"><pre class="highlight"><code>    1.countByKey
    2.countByValue
    3.collectAsMap
</code></pre>
</div>

<p>If you really do need every one of these values of the RDD and the data is too big to fit into memory, you can write out the RDD to files or export the RDD to a database that is large enough to hold all the data.</p>

<p>The article is <a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/dont_call_collect_on_a_very_large_rdd.html">HERE</a></p>
